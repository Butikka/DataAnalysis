{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "# from datetime import datetime, timedelta, date\n",
    "from datetime import datetime\n",
    "import time\n",
    "from polygon import RESTClient\n",
    "import logging\n",
    "import signal\n",
    "import sys\n",
    "import pickle\n",
    "import lz4.frame  # type: ignore\n",
    "import concurrent.futures\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nbimporter\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read pickle\n",
    "\n",
    "def read_pickle(file_path):\n",
    "    # Load the DataFrame from the compressed pickle file\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path, compression='gzip')\n",
    "        print(\"File loaded successfully!\")\n",
    "        # print(df.head())  # Display the first few rows\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all unique symbols in the 'symbol' column\n",
    "unique_symbols = df['symbol'].unique()\n",
    "print(unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter date range and symbols that are alerted by PEG model.\n",
    "def model_peg_list(data):\n",
    "    # symbol_filter = 'ABUS'  # Replace with the symbol you want to filter\n",
    "    date_filter = (data['timestamp'] >= '2022-01-01') & (data['timestamp'] <= '2024-09-30')\n",
    "    pegAlert = (data['pegAlert']==1)\n",
    "\n",
    "     # Apply filters and select unique values from the 'symbol' column\n",
    "    modelPEGlist = data[date_filter & pegAlert]['symbol'].unique()\n",
    "    \n",
    "    return modelPEGlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and sort the original DataFrame using the list of unique symbols\n",
    "def filter_data_by_symbols():\n",
    "    # Define file path\n",
    "    file_path = \"C:\\\\Users\\\\SamuliMustonen\\\\Documents\\\\Ready Solutions\\\\Docs\\\\StockTrading\\\\Data\\\\rawComplete\\\\all_raw_data_2022-01-01_to_2024-09-30.pickle.gz\"\n",
    "\n",
    "    # Variable for file reading function and use the dataframe\n",
    "    df = read_pickle(file_path)\n",
    "\n",
    "    # Run model peg list to create symbol list\n",
    "    symbols_list = model_peg_list(df)\n",
    "    \n",
    "    # Filter the DataFrame to include only rows where 'symbol' is in the symbols_list\n",
    "    filtered_df = df[df['symbol'].isin(symbols_list)]\n",
    "\n",
    "    # Sort the filtered DataFrame by 'symbol' and 'timestamp' (or another column if needed)\n",
    "    sorted_df = filtered_df.sort_values(by=['timestamp'])\n",
    "\n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define file path\n",
    "# file_path = \"C:\\\\Users\\\\SamuliMustonen\\\\Documents\\\\Ready Solutions\\\\Docs\\\\StockTrading\\\\Data\\\\rawComplete\\\\all_raw_data_2022-01-01_to_2024-09-30.pickle.gz\"\n",
    "# # Variable for file reading function\n",
    "# data = read_pickle(file_path)\n",
    "# # Convert to DataFrame\n",
    "# print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Main function to run the process\n",
    "def main():\n",
    "    \n",
    "    df = filter_data_by_symbols() # Data filtered by unique symbols\n",
    "    # if sorted_df is not None:\n",
    "    #     print(sorted_df.head())\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for a specific symbol and date range within 2023\n",
    "# symbol_filter = 'AGL'  # Replace with the symbol you want to filter\n",
    "# date_filter = (df['timestamp'] >= '2023-01-01') & (df['timestamp'] <= '2023-12-31')\n",
    "# openHigher = (df['openHigher'] == 1)\n",
    "# accVolume = (df['accVolume']==1)\n",
    "# uptrend = (df['uptrend']==1)\n",
    "# excCandle = (df['exceptionalCandleSize']==1)\n",
    "# closeHigh = (df['closeHigh']==1)\n",
    "\n",
    "# # Apply all filters at once\n",
    "# filtered_df = df[\n",
    "#     # (df['symbol'] == symbol_filter) & \n",
    "#     date_filter & \n",
    "#     openHigher & \n",
    "#     accVolume & \n",
    "#     uptrend & \n",
    "#     excCandle &\n",
    "#     closeHigh\n",
    "# ]\n",
    "\n",
    "# # View the filtered DataFrame\n",
    "# print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: C:\\Users\\SamuliMustonen\\Documents\\Ready Solutions\\Docs\\StockTrading\\Data\\modelPEG\\model_PEG_alert_list_2022-01-01_to_2024-09-30_20241021_182720.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the alert list to csv.\n",
    "start_date = \"2022-01-01\"  \n",
    "end_date = \"2024-09-30\"    \n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')  # current timestamp\n",
    "\n",
    "save_path = \"C:\\\\Users\\\\SamuliMustonen\\\\Documents\\\\Ready Solutions\\\\Docs\\\\StockTrading\\\\Data\\\\modelPEG\"\n",
    "file_name = f\"model_PEG_alert_list_{start_date}_to_{end_date}_{timestamp}.csv\"\n",
    "full_path = f\"{save_path}\\\\{file_name}\"\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(full_path, index=False)\n",
    "\n",
    "print(f\"File saved to: {full_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
