{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch raw data aggregations of daily stock data for the ticker list from polygon.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "# from datetime import datetime, timedelta, date\n",
    "import datetime\n",
    "import time\n",
    "from polygon import RESTClient\n",
    "import logging\n",
    "import signal\n",
    "import sys\n",
    "import pickle\n",
    "import lz4.frame  # type: ignore\n",
    "import concurrent.futures\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API details\n",
    "api_key = 'xxx' # add your api key but preferred to save it into environment\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# Define the path where the files will be saved\n",
    "save_path = \"C:\\\\Users\\\\user\\\\Data\\\\rawAggs\" # example saving path on device.\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Signal handler for graceful shutdown\n",
    "def signal_handler(sig, frame):\n",
    "    print(\"You pressed Ctrl+C!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# Function to retrieve and save aggregate data for a given ticker over a date range\n",
    "def get_aggs_for_ticker(ticker, start_date, end_date):\n",
    "    \"\"\"Retrieve aggregates for a given ticker and date range\"\"\"\n",
    "    client = RESTClient(api_key=api_key)  # Initialize client\n",
    "\n",
    "    aggs = []\n",
    "    for day in weekdays_between(start_date, end_date):\n",
    "        for a in client.list_aggs(ticker, 1, \"day\", day, day, limit=50000):\n",
    "            aggs.append(a)\n",
    "\n",
    "    # Save the data to a compressed .pickle.lz4 file. We wan it compressed as there might be a lot of data.\n",
    "    filename = os.path.join(save_path, f\"{ticker}-aggs-{start_date}_to_{end_date}.pickle.lz4\")\n",
    "    with open(filename, \"wb\") as file:\n",
    "        try:\n",
    "            compressed_data = lz4.frame.compress(pickle.dumps(aggs))\n",
    "            file.write(compressed_data)\n",
    "        except TypeError as e:\n",
    "            logging.error(f\"Serialization Error for {ticker}: {e}\")\n",
    "\n",
    "    logging.info(f\"Downloaded aggs for {ticker} from {start_date} to {end_date} and saved to {filename}\")\n",
    "\n",
    "# Function to generate all weekdays between two dates\n",
    "def weekdays_between(start_date, end_date):\n",
    "    \"\"\"Generate all weekdays between start_date and end_date\"\"\"\n",
    "    day = start_date\n",
    "    while day <= end_date:\n",
    "        if day.weekday() < 5:  # Only Monday to Friday\n",
    "            yield day\n",
    "        day += timedelta(days=1)\n",
    "\n",
    "def main():\n",
    "    start_date = datetime.date(2022, 1, 1)  # Start date: Beginning of 2022\n",
    "    end_date = datetime.date(2024, 9, 30)   # End date: 2024-09-30\n",
    "\n",
    "    # Load tickers from CSV file (assumes the CSV file has columns 'Ticker' and 'Market-Cap')\n",
    "    csv_file_path = \"C:\\\\Users\\\\user\\\\filename.csv\"  # Update this path with your actual file location. Same location as dataTickers save path.\n",
    "    df_tickers = pd.read_csv(csv_file_path, delimiter=',', header=0)\n",
    "\n",
    "    # Extract tickers from the 'Ticker' column\n",
    "    symbols = df_tickers['Ticker'].tolist()\n",
    "\n",
    "    # Use ThreadPoolExecutor to download data for each ticker in parallel. No idea what this is but seems important ;P.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(get_aggs_for_ticker, symbol, start_date, end_date) for symbol in symbols]\n",
    "        \n",
    "        # Optional: to make sure each task completes\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # This will raise any exceptions that occurred during execution\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error fetching data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
